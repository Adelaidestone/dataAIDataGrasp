"""
æ™ºèƒ½äº§å“æ•°æ®å¤„ç†å™¨ - Smart Product Data Processor
åŠŸèƒ½ï¼šæ™ºèƒ½å¤„ç†äº§å“HTMLæ•°æ®æ–‡ä»¶
- å•ä¸ªäº§å“æ¨¡å¼ï¼šè¾“å…¥æ–‡ä»¶å¤¹ç›´æ¥åŒ…å«HTMLæ–‡ä»¶
- æ‰¹é‡äº§å“æ¨¡å¼ï¼šè¾“å…¥æ–‡ä»¶å¤¹åŒ…å«å¤šä¸ªäº§å“å­æ–‡ä»¶å¤¹
è‡ªåŠ¨æ£€æµ‹æ¨¡å¼å¹¶å¤„ç†ï¼Œç”Ÿæˆæœ€ç»ˆèšåˆæ•°æ®
"""

import os
import json
import shutil
import subprocess
import sys
from datetime import datetime
import re
from pathlib import Path

import glob

class SmartProductProcessor:
    def __init__(self, base_input_path, base_output_path="E:\\dataAI\\batch_results"):
        self.base_input_path = base_input_path
        self.base_output_path = base_output_path
        self.script_mappings = {
            'main_allplatform': 'Grabbed_Aggregated_Analytics_Data.py',
            'user_retention': 'User_Retention_Scraper.py',
            'user_behavior': 'User_Behavior_Scraper.py',
            'revenue': 'Revenue_Scraper.py'
        }
    
    def get_folders_to_process(self):
        """è·å–éœ€è¦å¤„ç†çš„æ–‡ä»¶å¤¹åˆ—è¡¨"""
        folders = []
        try:
            for item in os.listdir(self.base_input_path):
                item_path = os.path.join(self.base_input_path, item)
                if os.path.isdir(item_path):
                    folders.append(item_path)
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
        
        return folders
    
    def identify_file_type(self, filename):
        """è¯†åˆ«HTMLæ–‡ä»¶ç±»å‹"""
        filename_lower = filename.lower()
        if 'ç•™å­˜' in filename or 'retention' in filename_lower:
            return 'user_retention'
        elif 'è¡Œä¸º' in filename or 'behavior' in filename_lower or 'behaviour' in filename_lower:
            return 'user_behavior'
        elif 'æ”¶å…¥' in filename or 'revenue' in filename_lower:
            return 'revenue'
        elif 'allplatform' in filename_lower or 'å…¨å¹³å°' in filename or 'ä¸‹è½½é‡' in filename:
            return 'main_allplatform'
        else:
            return 'unknown'
    
    def extract_product_name(self, filename):
        """ä»æ–‡ä»¶åæå–äº§å“åç§°"""
        # ç®€å•çš„äº§å“åæå–ï¼Œå¯ä»¥æ ¹æ®éœ€è¦ä¼˜åŒ–
        name_parts = filename.replace('.html', '').split('_')
        if name_parts:
            return name_parts[0]
        return "Unknown Product"
    
    def analyze_folder_files(self, folder_path):
        """åˆ†ææ–‡ä»¶å¤¹ä¸­çš„HTMLæ–‡ä»¶"""
        html_files = []
        
        for file in os.listdir(folder_path):
            if file.endswith('.html'):
                file_type = self.identify_file_type(file)
                product_name = self.extract_product_name(file)
                
                html_files.append({
                    'filename': file,
                    'filepath': os.path.join(folder_path, file),
                    'file_type': file_type,
                    'product_name': product_name,
                    'script': self.script_mappings.get(file_type, 'unknown')
                })
                
                print(f"  ğŸ“„ {file} â†’ {self.script_mappings.get(file_type, 'æœªçŸ¥è„šæœ¬')}")
        
        return html_files
    
    def update_script_path(self, script_name, files):
        """æ›´æ–°è„šæœ¬ä¸­çš„HTMLæ–‡ä»¶è·¯å¾„"""
        try:
            script_path = os.path.join("E:\\dataAI", script_name)
            backup_path = script_path + ".backup"
            
            # å¤‡ä»½åŸæ–‡ä»¶
            shutil.copy2(script_path, backup_path)
            
            # è¯»å–è„šæœ¬å†…å®¹
            with open(script_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if script_name in ['User_Retention_Scraper.py', 'User_Behavior_Scraper.py']:
                # å¤šå¹³å°è„šæœ¬ - æ›´æ–°html_fileså­—å…¸
                android_file = None
                ios_file = None
                
                for file_info in files:
                    if 'android' in file_info['filename'].lower() or 'å®‰å“' in file_info['filename']:
                        android_file = file_info['filepath'].replace('\\', '\\\\')
                    elif 'ios' in file_info['filename'].lower() or 'è‹¹æœ' in file_info['filename']:
                        ios_file = file_info['filepath'].replace('\\', '\\\\')
                    else:
                        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å¹³å°æ ‡è¯†ï¼Œé»˜è®¤ä¸ºAndroid
                        if not android_file:
                            android_file = file_info['filepath'].replace('\\', '\\\\')
                
                if android_file or ios_file:
                    # æ„å»ºæ–°çš„html_fileså­—å…¸
                    new_html_files = "html_files = {\n"
                    if android_file:
                        new_html_files += f'    "Android": r"{android_file}",\n'
                    if ios_file:
                        new_html_files += f'    "iOS": r"{ios_file}"\n'
                    new_html_files += "}"
                    
                    # æ›¿æ¢html_fileså­—å…¸
                    pattern = r'html_files\s*=\s*\{[^}]*\}'
                    content = re.sub(pattern, new_html_files, content, flags=re.DOTALL)
            
            else:
                # å•å¹³å°è„šæœ¬ - æ›´æ–°html_file_pathå˜é‡
                if files:
                    file_path = files[0]['filepath'].replace('\\', '\\\\')
                    pattern = r'html_file_path\s*=\s*r?"[^"]*"'
                    replacement = f'html_file_path = r"{file_path}"'
                    content = re.sub(pattern, replacement, content)
            
            # å†™å›æ–‡ä»¶
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"âœ… æ›´æ–°è„šæœ¬è·¯å¾„: {script_name}")
            return True
            
        except Exception as e:
            print(f"âŒ æ›´æ–°è„šæœ¬è·¯å¾„å¤±è´¥ {script_name}: {e}")
            return False
    
    def restore_script_backup(self, script_name):
        """æ¢å¤è„šæœ¬å¤‡ä»½"""
        try:
            script_path = os.path.join("E:\\dataAI", script_name)
            backup_path = script_path + ".backup"
            
            if os.path.exists(backup_path):
                shutil.copy2(backup_path, script_path)
                os.remove(backup_path)
                print(f"ğŸ”„ æ¢å¤è„šæœ¬: {script_name}")
            
        except Exception as e:
            print(f"âŒ æ¢å¤è„šæœ¬å¤±è´¥ {script_name}: {e}")
    
    def run_script(self, script_name):
        """è¿è¡Œè„šæœ¬"""
        try:
            script_path = os.path.join("E:\\dataAI", script_name)
            print(f"ğŸš€ è¿è¡Œ: {script_name}")
            
            result = subprocess.run([sys.executable, script_path], 
                                  cwd="E:\\dataAI")
            
            if result.returncode == 0:
                print(f"âœ… {script_name} è¿è¡ŒæˆåŠŸ")
                return True
            else:
                print(f"âŒ {script_name} è¿è¡Œå¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ è¿è¡Œè„šæœ¬å¤±è´¥ {script_name}: {e}")
            return False
    
    def process_revenue_files_separately(self, files):
        """å•ç‹¬å¤„ç†Revenueæ–‡ä»¶"""
        for file_info in files:
            print(f"ğŸš€ å¤„ç†Revenueæ–‡ä»¶: {file_info['filename']}")
            if self.update_script_path('Revenue_Scraper.py', [file_info]):
                self.run_script('Revenue_Scraper.py')
                self.restore_script_backup('Revenue_Scraper.py')
    
    def process_product_folder(self, product_folder_path):
        """å¤„ç†å•ä¸ªäº§å“æ–‡ä»¶å¤¹"""
        try:
            folder_name = os.path.basename(product_folder_path)
            print(f"ğŸ” åˆ†æäº§å“æ–‡ä»¶å¤¹: {folder_name}")
            
            # åˆ†ææ–‡ä»¶å¤¹ä¸­çš„HTMLæ–‡ä»¶
            html_files = self.analyze_folder_files(product_folder_path)
            
            if not html_files:
                print(f"âš ï¸ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰HTMLæ–‡ä»¶: {folder_name}")
                return False
            
            # æŒ‰è„šæœ¬åˆ†ç»„å¤„ç†
            script_groups = {}
            for file_info in html_files:
                script = file_info['script']
                if script != 'unknown':
                    if script not in script_groups:
                        script_groups[script] = []
                    script_groups[script].append(file_info)
            
            print(f"ğŸ”§ éœ€è¦è¿è¡Œ {len(script_groups)} ä¸ªè„šæœ¬")
            
            # å¤„ç†æ¯ä¸ªè„šæœ¬ç»„
            for script, files in script_groups.items():
                print(f"ğŸš€ å¤„ç†è„šæœ¬: {script}")
                
                if script == 'Revenue_Scraper.py':
                    # Revenueè„šæœ¬éœ€è¦å•ç‹¬å¤„ç†æ¯ä¸ªå¹³å°
                    self.process_revenue_files_separately(files)
                else:
                    # å…¶ä»–è„šæœ¬æ”¯æŒåŒå¹³å°
                    if self.update_script_path(script, files):
                        self.run_script(script)
                        self.restore_script_backup(script)
            
            # ç”Ÿæˆæœ€ç»ˆèšåˆæ•°æ®å¹¶ä¿å­˜ä¸ºå•ç‹¬çš„äº§å“æ–‡ä»¶
            self.generate_final_aggregated_data()
            self.save_product_data_separately()
            
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†äº§å“æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
            return False
    
    def generate_final_aggregated_data(self):
        """ç”Ÿæˆæœ€ç»ˆèšåˆæ•°æ®å¹¶æ¸…ç†åŸå§‹æ–‡ä»¶"""
        print("ğŸ”„ ç”Ÿæˆæœ€ç»ˆèšåˆæ•°æ®...")
        
        try:
            # è¿è¡ŒData_Aggregatorç”Ÿæˆæœ€ç»ˆèšåˆæ•°æ®
            data_aggregator_path = os.path.join("E:\\dataAI", "Data_Aggregator.py")
            result = subprocess.run([sys.executable, data_aggregator_path, "--skip-scrapers"], 
                                  cwd="E:\\dataAI")
            
            if result.returncode == 0:
                print("âœ… æœ€ç»ˆèšåˆæ•°æ®ç”ŸæˆæˆåŠŸ")
                self.cleanup_raw_data()
            else:
                print("âŒ æ•°æ®æ•´åˆå¤±è´¥")
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæœ€ç»ˆèšåˆæ•°æ®æ—¶å‡ºé”™: {e}")
    
    def save_current_product_data(self):
        """ä¿å­˜å½“å‰äº§å“æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶"""
        try:
            import uuid
            temp_file = f"temp_product_{uuid.uuid4().hex[:8]}.json"
            temp_path = os.path.join("E:\\dataAI", temp_file)
            
            # å¤åˆ¶å½“å‰çš„èšåˆæ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
            comprehensive_file = os.path.join("E:\\dataAI", "Comprehensive_Aggregated_Analytics_Data.json")
            if os.path.exists(comprehensive_file):
                import shutil
                shutil.copy2(comprehensive_file, temp_path)
                print(f"ğŸ“‹ å½“å‰äº§å“æ•°æ®å·²ä¿å­˜åˆ°: {temp_file}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜äº§å“æ•°æ®å¤±è´¥: {e}")
    
    def merge_all_products_data(self, successful_products):
        """åˆå¹¶æ‰€æœ‰äº§å“çš„æ•°æ®åˆ°æœ€ç»ˆèšåˆæ–‡ä»¶"""
        print(f"\nğŸ”„ åˆå¹¶ {len(successful_products)} ä¸ªäº§å“çš„æ•°æ®...")
        
        try:
            # è¿è¡ŒData_Aggregatorç”Ÿæˆæœ€ç»ˆåˆå¹¶æ•°æ®
            print("ğŸš€ è¿è¡Œæ•°æ®æ•´åˆå™¨åˆå¹¶æ‰€æœ‰äº§å“æ•°æ®...")
            data_aggregator_path = os.path.join("E:\\dataAI", "Data_Aggregator.py")
            result = subprocess.run([sys.executable, data_aggregator_path, "--skip-scrapers"], 
                                  cwd="E\\dataAI")
            
            if result.returncode == 0:
                print("âœ… æ‰€æœ‰äº§å“æ•°æ®åˆå¹¶å®Œæˆ")
                
                # æ¸…ç†åŸå§‹æ•°æ®
                self.cleanup_raw_data()
            else:
                print("âŒ æ•°æ®åˆå¹¶å¤±è´¥")
                
        except Exception as e:
            print(f"âŒ åˆå¹¶äº§å“æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def save_product_data_separately(self):
        """å°†å½“å‰äº§å“æ•°æ®ä¿å­˜ä¸ºç‹¬ç«‹æ–‡ä»¶"""
        try:
            comprehensive_file = os.path.join("E:\\dataAI", "Comprehensive_Aggregated_Analytics_Data.json")
            if os.path.exists(comprehensive_file):
                # è¯»å–å½“å‰æ•°æ®
                with open(comprehensive_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è·å–äº§å“åç§°
                if isinstance(data, list) and len(data) > 0:
                    app_name = data[0].get('Application', 'Unknown_Product')
                elif isinstance(data, dict):
                    app_name = data.get('Application', 'Unknown_Product')
                else:
                    app_name = 'Unknown_Product'
                
                # æ¸…ç†äº§å“åç§°ï¼Œç”¨äºæ–‡ä»¶å
                import re
                clean_name = re.sub(r'[^\w\s-]', '', app_name).strip()
                clean_name = re.sub(r'[-\s]+', '_', clean_name)
                
                # ä¿å­˜ä¸ºç‹¬ç«‹çš„äº§å“æ–‡ä»¶
                product_file = f"Product_{clean_name}_Data.json"
                product_path = os.path.join("E:\\dataAI", product_file)
                
                import shutil
                shutil.copy2(comprehensive_file, product_path)
                print(f"ğŸ’¾ äº§å“æ•°æ®å·²ä¿å­˜: {product_file}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜ç‹¬ç«‹äº§å“æ•°æ®å¤±è´¥: {e}")
    
    def run_simple_data_separator(self):
        """è¿è¡Œç®€å•æ•°æ®åˆ†ç¦»å™¨"""
        try:
            separator_path = os.path.join("E:\\dataAI", "Simple_Data_Separator.py")
            if os.path.exists(separator_path):
                result = subprocess.run([sys.executable, separator_path], cwd="E:\\dataAI")
                if result.returncode == 0:
                    print("âœ… æ•°æ®åˆ†ç¦»æˆåŠŸ")
                else:
                    print("âŒ æ•°æ®åˆ†ç¦»å¤±è´¥")
            else:
                print("âŒ ç®€å•æ•°æ®åˆ†ç¦»å™¨ä¸å­˜åœ¨")
        except Exception as e:
            print(f"âŒ è¿è¡Œæ•°æ®åˆ†ç¦»å™¨å¤±è´¥: {e}")
    
    def cleanup_raw_data(self):
        """æ¸…ç†åŸå§‹æ•°æ®æ–‡ä»¶"""
        print("ğŸ§¹ æ¸…ç†åŸå§‹æ•°æ®æ–‡ä»¶...")
        
        try:
            main_dir = "E:\\dataAI"
            
            # åˆ é™¤åŸå§‹JSONæ–‡ä»¶
            json_patterns = [
                'Aggregated_Analytics_Data.json',
                'User_Behavior_*.json',
                'PolyBuzz_*.json'
            ]
            
            cleaned_files = 0
            for pattern in json_patterns:
                for file_path in glob.glob(os.path.join(main_dir, pattern)):
                    if os.path.basename(file_path) != 'Comprehensive_Aggregated_Analytics_Data.json':
                        os.remove(file_path)
                        cleaned_files += 1
                        print(f"ğŸ—‘ï¸ åˆ é™¤: {os.path.basename(file_path)}")
            
            print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_files} ä¸ªåŸå§‹æ–‡ä»¶")
            print("ğŸ“Š åªä¿ç•™: Comprehensive_Aggregated_Analytics_Data.json")
            
        except Exception as e:
            print(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    

    def process_all_folders(self):
        """æ™ºèƒ½å¤„ç†è¾“å…¥æ–‡ä»¶å¤¹ - è‡ªåŠ¨åˆ¤æ–­å•ä¸ªäº§å“è¿˜æ˜¯å¤šä¸ªäº§å“"""
        
        
        # å…ˆæ£€æŸ¥è¾“å…¥æ–‡ä»¶å¤¹æ˜¯å¦ç›´æ¥åŒ…å«HTMLæ–‡ä»¶
        html_files_in_root = [f for f in os.listdir(self.base_input_path) 
                             if f.endswith('.html') and os.path.isfile(os.path.join(self.base_input_path, f))]
        
        if html_files_in_root:
            # å¦‚æœæ ¹ç›®å½•ç›´æ¥åŒ…å«HTMLæ–‡ä»¶ï¼Œè¯´æ˜è¿™æ˜¯å•ä¸ªäº§å“æ–‡ä»¶å¤¹
            print("ğŸ¯ æ£€æµ‹åˆ°å•ä¸ªäº§å“æ¨¡å¼ - è¾“å…¥æ–‡ä»¶å¤¹ç›´æ¥åŒ…å«HTMLæ–‡ä»¶")
            print("=" * 80)
            success = self.process_product_folder(self.base_input_path)
            
            if success:
                successful_products = [os.path.basename(self.base_input_path)]
                print(f"âœ… äº§å“å¤„ç†æˆåŠŸ")
            else:
                successful_products = []
                print(f"âŒ äº§å“å¤„ç†å¤±è´¥")
        else:
            # å¦‚æœæ ¹ç›®å½•ä¸åŒ…å«HTMLæ–‡ä»¶ï¼Œè¯´æ˜æ˜¯å¤šä¸ªäº§å“æ–‡ä»¶å¤¹çš„æ‰¹é‡æ¨¡å¼
            folders = self.get_folders_to_process()
            
            if not folders:
                print(f"âŒ è¾“å…¥æ–‡ä»¶å¤¹ä¸­æ—¢æ²¡æœ‰HTMLæ–‡ä»¶ï¼Œä¹Ÿæ²¡æœ‰å­æ–‡ä»¶å¤¹: {self.base_input_path}")
                return
            
            print(f"ğŸ¯ æ£€æµ‹åˆ°æ‰¹é‡äº§å“æ¨¡å¼ - æ‰¾åˆ° {len(folders)} ä¸ªäº§å“æ–‡ä»¶å¤¹")
            print("=" * 80)
            
            successful_products = []
            
            for i, folder_path in enumerate(folders, 1):
                folder_name = os.path.basename(folder_path)
                print(f"\nğŸš€ [{i}/{len(folders)}] å¼€å§‹å¤„ç†äº§å“: {folder_name}")
                print("=" * 80)
                
                try:
                    success = self.process_product_folder(folder_path)
                    if success:
                        successful_products.append(folder_name)
                        print(f"âœ… äº§å“ '{folder_name}' å¤„ç†æˆåŠŸ")
                    else:
                        print(f"âŒ äº§å“ '{folder_name}' å¤„ç†å¤±è´¥")
                except Exception as e:
                    print(f"âŒ å¤„ç†äº§å“æ–‡ä»¶å¤¹ '{folder_name}' æ—¶å‡ºé”™: {e}")
                    continue
        
        # å¤„ç†å®Œæ‰€æœ‰äº§å“åï¼Œä½¿ç”¨ç®€å•æ•°æ®åˆ†ç¦»å™¨
        if successful_products:
            print(f"\nğŸ”„ ä½¿ç”¨ç®€å•æ•°æ®åˆ†ç¦»å™¨å¤„ç† {len(successful_products)} ä¸ªäº§å“æ•°æ®...")
            self.run_simple_data_separator()
        
        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        self.generate_batch_summary(successful_products)
    
    def generate_batch_summary(self, successful_products):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†çš„æ€»ç»“æŠ¥å‘Š"""
        summary = {
            "Batch_Processing_Summary": {
                "Processing_Time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "Base_Input_Path": self.base_input_path,
                "Base_Output_Path": "E:\\dataAI",  # æœ€ç»ˆèšåˆæ•°æ®çš„ä½ç½®
                "Successful_Products": successful_products,
                "Total_Products_Processed": len(successful_products),
                "Final_Output_Files": []
            }
        }
        
        # æ”¶é›†æœ€ç»ˆèšåˆæ•°æ®æ–‡ä»¶
        main_dir = "E:\\dataAI"
        final_files = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆèšåˆæ•°æ®æ–‡ä»¶
        comprehensive_file = os.path.join(main_dir, "Comprehensive_Aggregated_Analytics_Data.json")
        if os.path.exists(comprehensive_file):
            final_files.append("Comprehensive_Aggregated_Analytics_Data.json")
        
        summary["Batch_Processing_Summary"]["Final_Output_Files"] = final_files
        
        # ä¿å­˜æ€»ç»“æŠ¥å‘Š
        summary_path = os.path.join(main_dir, 'Batch_Processing_Summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=4)
        
        print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†äº† {len(successful_products)} ä¸ªäº§å“:")
        for product in successful_products:
            print(f"   âœ… {product}")
        print(f"ğŸ“Š æ€»ç»“æŠ¥å‘Š: {summary_path}")
        print(f"ğŸ“ æœ€ç»ˆèšåˆæ•°æ®: {main_dir}")

def main():
    """ä¸»å‡½æ•°"""
    
    # ========================================
    # ğŸ”§ åœ¨è¿™é‡Œè®¾ç½®æ‚¨çš„è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
    # ========================================
    INPUT_FOLDER = r"D:\Users\Mussy\Desktop\input"  # ğŸ“‚ ä¿®æ”¹ä¸ºæ‚¨çš„è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
    
    # å¯é€‰ï¼šè®¾ç½®ä¸´æ—¶è¾“å‡ºè·¯å¾„ï¼ˆæœ€ç»ˆèšåˆæ•°æ®å§‹ç»ˆä¿å­˜åˆ° E:\dataAI\ï¼‰
    TEMP_OUTPUT = r"D:\Users\Mussy\Desktop\result"
    
    # ========================================
    
    if not os.path.exists(INPUT_FOLDER):
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {INPUT_FOLDER}")
        print("ğŸ’¡ è¯·åœ¨è„šæœ¬ä¸­ä¿®æ”¹ INPUT_FOLDER å˜é‡ä¸ºæ­£ç¡®çš„è·¯å¾„")
        return
    
    print("ğŸ¯ æ™ºèƒ½äº§å“æ•°æ®å¤„ç†å™¨")
    print("=" * 60)
    print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {INPUT_FOLDER}")
    print(f"ğŸ“Š æœ€ç»ˆèšåˆæ•°æ®ä½ç½®: E:\\dataAI\\")
    print(f"ğŸ“ ä¸´æ—¶å¤„ç†ç›®å½•: {TEMP_OUTPUT}")
    print("ğŸ¤– è‡ªåŠ¨æ£€æµ‹: å•ä¸ªäº§å“ æˆ– æ‰¹é‡äº§å“")
    print("=" * 60)
    
    processor = SmartProductProcessor(INPUT_FOLDER, TEMP_OUTPUT)
    processor.process_all_folders()

if __name__ == "__main__":
    main()