import pandas as pd
from bs4 import BeautifulSoup
from lxml import etree
import json
import re # Import regular expression module

# Define the application name explicitly as it's part of the filename, not in table data directly
# APPLICATION_NAME = "PolyBuzz: Chat with AI Friends"

def extract_product_name_from_html(soup):
    """
    Extract product name from HTML content
    """
    # Method 1: Try to extract from page title
    title_tag = soup.find('title')
    if title_tag:
        title_text = title_tag.get_text(strip=True)
        # Extract product name from title (assuming format like "ProductName | ç”¨æˆ·ç•™å­˜" or similar)
        if '|' in title_text:
            product_name = title_text.split('|')[0].strip()
            return product_name
        elif '_' in title_text:
            product_name = title_text.split('_')[0].strip()
            return product_name
    
    # Method 2: Try to extract from breadcrumb or navigation elements
    breadcrumb = soup.find('nav', class_='breadcrumb') or soup.find('div', class_='breadcrumb')
    if breadcrumb:
        links = breadcrumb.find_all('a')
        if len(links) > 1:
            return links[-2].get_text(strip=True)  # Usually the second to last is the app name
    
    # Method 3: Try to extract from header or h1 tags
    h1_tag = soup.find('h1')
    if h1_tag:
        h1_text = h1_tag.get_text(strip=True)
        if 'ç”¨æˆ·ç•™å­˜' not in h1_text and 'ç•™å­˜' not in h1_text:
            return h1_text
    
    # Method 4: Try to extract from meta tags
    meta_title = soup.find('meta', attrs={'name': 'title'}) or soup.find('meta', attrs={'property': 'og:title'})
    if meta_title:
        content = meta_title.get('content', '')
        if '|' in content:
            return content.split('|')[0].strip()
    
    # Method 5: Try to extract from specific app info elements
    app_info = soup.find('div', class_=lambda x: x and ('app-info' in x or 'product-info' in x))
    if app_info:
        app_name = app_info.find('span') or app_info.find('div')
        if app_name:
            return app_name.get_text(strip=True)
    
    return "Unknown Product"

def extract_app_info_from_html(soup):
    """
    Extract application name and channel information from HTML content
    """
    app_info = {
        "app_name": "Unknown App",
        "channel": "Unknown Channel"
    }
    
    # Method 1: Try to extract from page title
    title_tag = soup.find('title')
    if title_tag:
        title_text = title_tag.get_text(strip=True)
        if '|' in title_text:
            app_info["app_name"] = title_text.split('|')[0].strip()
        elif '_' in title_text:
            app_info["app_name"] = title_text.split('_')[0].strip()
    
    # Method 2: Try to extract channel from URL or meta information
    # Look for app store indicators in the HTML
    meta_tags = soup.find_all('meta')
    for meta in meta_tags:
        content = meta.get('content', '').lower()
        if 'google play' in content or 'android' in content:
            app_info["channel"] = "Google Play"
            break
        elif 'app store' in content or 'ios' in content:
            app_info["channel"] = "App Store"
            break
    
    # Method 3: Look for platform indicators in the HTML content
    html_text = soup.get_text().lower()
    if 'google play' in html_text or 'android' in html_text:
        if app_info["channel"] == "Unknown Channel":
            app_info["channel"] = "Google Play"
    elif 'app store' in html_text or 'ios' in html_text:
        if app_info["channel"] == "Unknown Channel":
            app_info["channel"] = "App Store"
    
    # Method 4: Try to extract from specific platform elements
    platform_elements = soup.find_all(['div', 'span'], class_=lambda x: x and ('platform' in x.lower() or 'store' in x.lower()))
    for element in platform_elements:
        text = element.get_text().lower()
        if 'google play' in text or 'android' in text:
            app_info["channel"] = "Google Play"
            break
        elif 'app store' in text or 'ios' in text:
            app_info["channel"] = "App Store"
            break
    
    return app_info

# Mapping for Chinese headers to English headers
HEADER_MAP = {
    'æœˆ': 'Month',
    'åº”ç”¨': 'Application',
    'ç¬¬0å¤©': 'Day 0 Retention',
    'ç¬¬1å¤©': 'Day 1 Retention',
    'ç¬¬2å¤©': 'Day 2 Retention',
    'ç¬¬3å¤©': 'Day 3 Retention',
    'ç¬¬4å¤©': 'Day 4 Retention',
    'ç¬¬5å¤©': 'Day 5 Retention',
    'ç¬¬6å¤©': 'Day 6 Retention',
    'ç¬¬7å¤©': 'Day 7 Retention',
    'ç¬¬14å¤©': 'Day 14 Retention',
    'ç¬¬30å¤©': 'Day 30 Retention',
}

def convert_to_numeric(value_str):
    if not isinstance(value_str, str) or not value_str.strip():
        return value_str # Return as is if not a string or empty

    cleaned_str = value_str.replace('$', '').replace('%', '').strip()

    is_negative = False
    if cleaned_str.startswith('-'):
        is_negative = True
        cleaned_str = cleaned_str[1:]

    multiplier = 1
    if 'äº¿' in cleaned_str:
        multiplier = 100_000_000
        cleaned_str = cleaned_str.replace('äº¿', '')
    elif 'ä¸‡' in cleaned_str:
        multiplier = 10_000
        cleaned_str = cleaned_str.replace('ä¸‡', '')
    elif 'åƒ' in cleaned_str:
        multiplier = 1_000
        cleaned_str = cleaned_str.replace('åƒ', '')
    
    # Remove any remaining non-numeric characters that might cause conversion errors
    cleaned_str = ''.join(filter(lambda x: x.isdigit() or x == '.', cleaned_str))

    try:
        if not cleaned_str: # Handle cases where after cleaning, string is empty
            return value_str
        numeric_value = float(cleaned_str) * multiplier
        if is_negative:
            numeric_value = -numeric_value
        return numeric_value # Return as float for percentages
    except ValueError:
        return value_str # Return original if conversion fails

# PLATFORM_COLOR_MAP is not needed for this retention table as it doesn't have line charts
PLATFORM_COLOR_MAP = {}

def extract_retention_table_data(table_wrapper, table_name):
    if not table_wrapper:
        print(f"Could not find the table wrapper for {table_name}.")
        return []

    # Initialize output for this specific table
    table_output_records = []

    # Extract headers
    headers = []

    # Check if it's the app_user_retention_table (monthly data) or publisher_apps_user_retention_table (overall app data)
    is_monthly_table = (table_wrapper.get('data-table-type') == 'app_user_retention_table')

    if is_monthly_table:
        # 'æœˆ' header for monthly table
        month_header_cell = table_wrapper.find('div', {'data-header-key': 'date'})
        if month_header_cell:
            span_content = month_header_cell.find('span', class_='Tooltip__ContentWrapper-sc-a710cec5-0')
            if span_content:
                headers.append(span_content.get_text(strip=True))
            else:
                headers.append(month_header_cell.get_text(strip=True))
    else:
        # 'åº”ç”¨' header for publisher apps table
        app_header_cell = table_wrapper.find('div', {'data-header-key': 'product_id'})
        if app_header_cell:
            headers.append(app_header_cell.get_text(strip=True))

    # Day retention headers (ç¬¬0å¤©, ç¬¬1å¤©, etc.)
    # Find the row containing day retention headers. This might be in a nested div.
    day_retention_headers_row = table_wrapper.find('div', class_=lambda x: x and 'TableHeader__CellRow-sc-194ff62d-2' in x.split() and 'hoHRDw' in x.split())
    if day_retention_headers_row:
        for cell in day_retention_headers_row.find_all('div', class_='TableHeader__CellContent-sc-194ff62d-3'):
            headers.append(cell.get_text(strip=True))

    print(f"Extracted headers (Chinese) for {table_name}: {headers}")
    print(f"Number of extracted headers (Chinese) for {table_name}: {len(headers)}")

    # Data extraction
    fixed_table_grid = table_wrapper.find('div', class_=lambda x: x and 'ReactVirtualized__Table' in x.split() and 'FixedStyledTable' in x.split())
    scrollable_table_grid = table_wrapper.find('div', class_=lambda x: x and 'ReactVirtualized__Table' in x.split() and 'StyledTable' in x.split() and 'FixedStyledTable' not in x.split())

    if fixed_table_grid and scrollable_table_grid:
        fixed_rows = fixed_table_grid.find_all('div', class_='ReactVirtualized__Table__row', attrs={'aria-rowindex': True})
        scrollable_rows = scrollable_table_grid.find_all('div', class_='ReactVirtualized__Table__row', attrs={'aria-rowindex': True})

        print(f"Fixed rows found for {table_name}: {len(fixed_rows)}")
        print(f"Scrollable rows found for {table_name}: {len(scrollable_rows)}")

        min_rows = min(len(fixed_rows), len(scrollable_rows))

        for i in range(min_rows):
            row_data = []
            
            # Extract app name or month based on table type
            if is_monthly_table:
                month_div = fixed_rows[i].find('div', {'data-testid': 'table-cell#date'})
                row_data.append(month_div.get_text(strip=True) if month_div else "")
            else:
                app_name_div = fixed_rows[i].find('div', {'data-testid': 'text-component'})
                row_data.append(app_name_div.get_text(strip=True) if app_name_div else "")

            # Extract day retention percentages
            scroll_row = scrollable_rows[i]
            for retention_day_key in [f'est_retention_day__aggr-{j}' for j in range(8)] + ['est_retention_day__aggr-14', 'est_retention_day__aggr-30']:
                retention_cell = scroll_row.find('div', {'data-key': retention_day_key})
                retention_value_str = "N/A"
                if retention_cell:
                    span_value = retention_cell.find('span', class_='DataMetric__DisplayValue-sc-a50818d6-1')
                    if span_value:
                        retention_value_str = span_value.get_text(strip=True)
                    else:
                        na_span = retention_cell.find('span', class_=lambda x: x and 'NA__Wrapper-sc-7d3243c2-0' in x.split())
                        if na_span:
                            retention_value_str = na_span.get_text(strip=True)
                
                row_data.append(convert_to_numeric(retention_value_str))
            
            table_output_records.append(row_data)
            print(f"Row {i} data length for {table_name}: {len(row_data)}")

    # Adjust headers to include only the desired ones and convert to English
    final_headers = []
    if headers:
        for h in headers:
            final_headers.append(HEADER_MAP.get(h, h))

    print(f"Final headers (English) for {table_name}: {final_headers}")
    print(f"Number of final headers (English) for {table_name}: {len(final_headers)}")

    if table_output_records and final_headers:
        df = pd.DataFrame(table_output_records, columns=final_headers)
        print(f"âœ… æˆåŠŸæå– {table_name} çš„è¡¨æ ¼æ•°æ®ï¼š")
        return df.to_dict(orient='records')
    else:
        print(f"No data to save for {table_name}.")
        return []

# HTML file paths for both platforms
html_files = {
    "Android": r"D:\\Users\\Mussy\\Desktop\\æ–°å»ºæ–‡ä»¶å¤¹\\PolyBuzz_ Chat with AI Friends _ ç”¨æˆ·ç•™å­˜.html",
    "iOS": r"D:\\Users\\Mussy\\Desktop\\æ–°å»ºæ–‡ä»¶å¤¹\\PolyBuzz_ Chat with AI Friends _ data.aiè‹¹æœç”¨æˆ·ç•™å­˜.html"
}

def process_html_file(file_path, platform_name):
    """
    Process a single HTML file and return the extracted data
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
    except FileNotFoundError:
        print(f"âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return None
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
        return None

    soup = BeautifulSoup(html_content, 'html.parser')
    
    print(f"\nğŸ” å¤„ç† {platform_name} å¹³å°æ•°æ®...")

    # Extract product name from HTML
    product_name = extract_product_name_from_html(soup)
    print(f"æå–åˆ°çš„äº§å“å: {product_name}")

    # Extract application info (name and channel) from HTML
    app_info = extract_app_info_from_html(soup)
    print(f"æå–åˆ°çš„åº”ç”¨å: {app_info['app_name']}")
    print(f"æå–åˆ°çš„æ¸ é“: {app_info['channel']}")

    # --- Extract data from the first table (Monthly App Retention) ---
    table_wrapper_monthly = soup.find('div', {'data-table-type': 'app_user_retention_table'})
    monthly_retention_data = extract_retention_table_data(table_wrapper_monthly, f"{platform_name} Monthly App Retention")

    # --- Extract data from the second table (Publisher Apps User Retention) ---
    table_wrapper_publisher = soup.find('div', {'data-table-type': 'publisher_apps_user_retention_table'})
    publisher_retention_data = extract_retention_table_data(table_wrapper_publisher, f"{platform_name} Publisher Apps User Retention (Overall)")

    return {
        "Application": product_name,
        "Platform": app_info['channel'],
        "Monthly App Retention": monthly_retention_data,
        "Publisher Apps User Retention (Overall)": publisher_retention_data
    }

# Process all HTML files
all_platform_data = {}
for platform, file_path in html_files.items():
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹å¤„ç† {platform} å¹³å°...")
    print(f"{'='*60}")
    
    platform_data = process_html_file(file_path, platform)
    if platform_data:
        all_platform_data[platform] = platform_data
        print(f"âœ… {platform} å¹³å°æ•°æ®å¤„ç†å®Œæˆ")
    else:
        print(f"âŒ {platform} å¹³å°æ•°æ®å¤„ç†å¤±è´¥")

# Save data for each platform separately and create a combined file
print(f"\n{'='*60}")
print("ğŸ’¾ ä¿å­˜æ•°æ®æ–‡ä»¶...")
print(f"{'='*60}")

for platform, data in all_platform_data.items():
    if platform == "Android":
        output_path = "PolyBuzz_User_Retention_Aggregated_Analytics_Data.json"
    else:
        output_path = f"PolyBuzz_User_Retention_{platform}_Aggregated_Analytics_Data.json"
    
    try:
        with open(output_path, 'w', encoding='utf-8') as json_file:
            json.dump(data, json_file, ensure_ascii=False, indent=4)
        print(f"âœ… {platform} æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ {platform} æ•°æ®æ—¶å‡ºé”™: {e}")

# Create a combined summary file
if all_platform_data:
    combined_data = {
        "Application": next(iter(all_platform_data.values()))["Application"],
        "Platforms": all_platform_data
    }
    
    combined_output_path = "PolyBuzz_User_Retention_Combined_Analytics_Data.json"
    try:
        with open(combined_output_path, 'w', encoding='utf-8') as json_file:
            json.dump(combined_data, json_file, ensure_ascii=False, indent=4)
        print(f"âœ… åˆå¹¶æ•°æ®å·²ä¿å­˜åˆ°: {combined_output_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆå¹¶æ•°æ®æ—¶å‡ºé”™: {e}")

print(f"\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç†äº† {len(all_platform_data)} ä¸ªå¹³å°çš„æ•°æ®")
